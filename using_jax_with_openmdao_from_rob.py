# -*- coding: utf-8 -*-
"""Using Jax with OpenMDAO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qoUGMSGRXheKw-xEHsKpImRFVgp6k5Io
"""

# !python -m pip install git+https://github.com/robfalck/OpenMDAO.git@math_subpackage

from functools import partial

import jax
import jax.numpy as jnp

import openmdao.api as om
# from openmdao.math import act_tanh

# This is in here as an attempt to get more precision out of jax
jax.config.update("jax_enable_x64", True)


#
# def act_tanh(x, mu=1.0E-2, z=0., a=-1., b=1.):
#     """
#     Differentiable activation function based on the hyperbolic tangent.
#
#     Parameters
#     ----------
#     x : float or np.array
#         The input at which the value of the activation function
#         is to be computed.
#     mu : float
#         A shaping parameter which impacts the "abruptness" of
#         the activation function. As this value approaches zero
#         the response approaches that of a step function.
#     z : float
#         The value of the independent variable about which the
#         activation response is centered.
#     a : float
#         The initial value that the input asymptotically approaches
#         as x approaches negative infinity.
#     b : float
#         The final value that the input asymptotically approaches
#         as x approaches positive infinity.
#
#     Returns
#     -------
#     float or np.array
#         The value of the activation response at the given input.
#     """
#     dy = b - a
#     tanh_term = np.tanh((x - z) / mu)
#     return 0.5 * dy * (1 + tanh_term) + a
#
def act_tanh(x, mu=1.0E-2, z=0., a=-1., b=1.):
    """
    Differentiable activation function based on the hyperbolic tangent.

    act_tanh can be used to approximate a step function from `a` to `b`, occurring at x=z.
    Smaller values of parameter `mu` more accurately represent a step function but the "sharpness" of the corners in the
    response may be more difficult for gradient-based approaches to resolve.

    Parameters
    ----------
    x : float or jnp.array
        The input at which the value of the activation function
        is to be computed.
    mu : float
        A shaping parameter which impacts the "abruptness" of
        the activation function. As this value approaches zero
        the response approaches that of a step function.
    z : float
        The value of the independent variable about which the
        activation response is centered.
    a : float
        The initial value that the input asymptotically approaches
        as x approaches negative infinity.
    b : float
        The final value that the input asymptotically approaches
        as x approaches positive infinity.

    Returns
    -------
    float or jnp.array
        The value of the activation response at the given input.
    """
    dy = b - a
    tanh_term = jnp.tanh((x - z) / mu)
    return 0.5 * dy * (1 + tanh_term) + a


class CountingComp(om.ExplicitComponent):
    
    def initialize(self):
        self.options.declare('vec_size', types=(int,))
        self.options.declare('threshold', types=(float,), default=0.0)
        self.options.declare('mu', types=(float,), default=0.01)
    
    def setup(self):
        n = self.options['vec_size']
        self.add_input('x', shape=(n,))
        self.add_output('count', shape=(1,))
        
        # The partials are a dense row in this case (1 row x N inputs)
        # There is no need to specify a sparsity pattern.
        self.declare_partials(of='count', wrt='x')

    @partial(jax.jit, static_argnums=(0,))
    # @partial(jax.jit)
    def _compute_partials_jacfwd(self, x):
        deriv_func = jax.jacfwd(self.compute_primal, argnums=[0])
        dx, = deriv_func(x)
        return dx
    
    @partial(jax.jit, static_argnums=(0,))
    # @partial(jax.jit)
    def _compute_partials_jacrev(self, x):
        deriv_func = jax.jacrev(self.compute_primal, argnums=[0])
        # Always returns a tuple
        dx, = deriv_func(x)
        return dx

    @partial(jax.jit, static_argnums=(0,))
    # @partial(jax.jit)
    def _compute_partials_jvp(self, x):
        # Note that JVP is a poor choice here, since the jacobian is a row vector!
        # We have to call it once with each individual element in x set to 1.0
        # while all the others are zero in order to get a correct result!
        
        # jvp always returns the primal and the jvp
        # This will give incorrect results! There is "cross-talk" amongs the different
        # indices in the tangents.
        _, dx = jax.jvp(self.compute_primal,
                        primals=(x,),
                        tangents=(jnp.ones_like(x),))
        return dx

    # TODO: how to properly use vjp?
    @partial(jax.jit, static_argnums=(0,))
    # @partial(jax.jit)
    def _compute_partials_vjp(self, x):
        # Note that JVP is a poor choice here, since the jacobian is a row vector!
        # We have to call it once with each individual element in x set to 1.0
        # while all the others are zero in order to get a correct result!
        
        # vjp always returns the primal and the vjp
        _, vjp_fun = jax.vjp(self.compute_primal, x)
        dx = vjp_fun(self.compute_primal(x))
        return dx
 
    # TODO: how to use jax.jit and get self.<attribute> into the function here? Wrapped function?
    # @partial(jax.jit)
    @partial(jax.jit, static_argnums=(0,))
    def compute_primal(self, x):
        mu = self.options['mu']
        z = self.options['threshold']
        return jnp.sum(act_tanh(x, mu, z, 0.0, 1.0))
    
    def compute(self, inputs, outputs):
        z = self.options['threshold']
        x = inputs['x']
        mu = self.options['mu']
        
        outputs['count'] = self.compute_primal(*inputs.values())
        
    def compute_partials(self, inputs, partials):
        dx = self._compute_partials_jvp(*inputs.values())

        partials['count', 'x'] = dx

    def _tree_flatten(self):
        children = tuple()  # arrays / dynamic values
        aux_data = {'options': self.options}  # static values
        return (children, aux_data)

    @classmethod
    def _tree_unflatten(cls, aux_data, children):
        return cls(*children, **aux_data)

from jax import tree_util
tree_util.register_pytree_node(CountingComp,
                               CountingComp._tree_flatten,
                               CountingComp._tree_unflatten)

import numpy as np

N = 10

p = om.Problem()
p.model.add_subsystem('counter',
                      CountingComp(vec_size=N,threshold=0.5, mu=0.01),
                      promotes_inputs=['x'], promotes_outputs=['count'])
p.setup(force_alloc_complex=True)
p.set_val('x', np.linspace(0, 1, N))
p.run_model()
p.model.list_inputs(print_arrays=True)
p.model.list_outputs(print_arrays=True)

with np.printoptions(linewidth=1024):
    p.check_partials(method='cs', compact_print=False);

